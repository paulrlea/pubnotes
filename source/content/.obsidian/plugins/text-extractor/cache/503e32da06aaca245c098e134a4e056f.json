{"path":"Notes/Physics/Computing for Physicists/In Class 6/CFP_lec6_2024.pdf","text":"Lecture 11: Parallel computing ‚ó¶Introduction to parallel computing ‚ó¶Introduction to parallel computing ‚ó¶Introduction to parallel computing ‚ó¶Introduction to parallel computing ‚ó¶OpenMP vs MPI ‚ó¶Job queue, job submission A parallel computer RAM Bus RAM Bus RAM Bus RAM Bus RAM Bus The most powerful supercomputer (as of today) consists of 8,699,904 cores and has 4 times as many GPUs than CPUs symmetric multiprocessor (SMP) system shared memory system or network switch distributed memory system Each node can also have one or more graphics processing units (GPUs). Parallel computing methods/interfaces ÔÅ± OpenMPis a set of directives (add-ons to a compiler), libraries and environment variables, which allows to program for shared memory systems. ÔÅ± Message Passing Interface (MPI) is a standardized library allowing users to write parallel programs for both shared-memoryand distributed-memorysystems. ÔÉò MPI is currently the standard for using nearly all supercomputers. ÔÉò MPI is available for C, C++, and Fortran 77/90/95, and Python ÔÉò MPI is a standard, not an implementation. There are a variety of implementations, some are available for free (OpenMPI, MVAPICH2, ‚Ä¶), some are provided by hardware vendors (Intel, IBM, ‚Ä¶) ÔÅ± CUDAis a special interface/library for using NVIDIA GPUs. MPI on your laptop Windows environment Linux (Debian/Ubuntu environment) sudo apt install package_name execute command as superuser package manager Looks for installable packages with ‚Äúpackage_name‚Äù in the respositories. Install openmpi: sudo apt install openmpi-bin sudo apt install libopenmpi-dev pip install mpi4py How an MPI program gets excecuted? MPI_COMM_SIZE MPI_COMM_RANK tid=0 process 0 MPI_INIT MPI_FINALIZE do something ‚Ä¶ start MPI program stop MPI program MPI_COMM_SIZE MPI_COMM_RANK tid=1 process 1 MPI_INIT MPI_FINALIZE do something ‚Ä¶ MPI_COMM_SIZE MPI_COMM_RANK tid=N-1 process N-1 MPI_INIT MPI_FINALIZE do something ‚Ä¶ Python example Simple MPI program Running on 2 cores: Running on 4 cores: mpi4py python library Job server (slurm job server) sinfo: list of job queues and available nodes squeue: current job queue, list of running and waiting jobs. sbatch: submit a job to the queue using a script. The script specifies how many nodes/cores to run on, which queue to submit to, time allowed to run, output file name, etc. run using, sbatch myscript scancel: cancels a submitted job with a specified JobID. How to pass messages (point to point)? C/C++: MPI_Send(&buffer,count,datatype,destination,tag,comm) MPI_Recv(&buffer,count,datatype,source,tag,comm,&status) buffer: the message to be sent count: the number of data elements with a specified datatype datatype: an MPI data type (next slide) destination: the destination (a task id) of the message for sending source: the source (a task id) of the message for receiving tag: a tag of the message (like a shipping/packing slip from Amazon) comm: the MPI communicator, usually the MPI_COMM_WORLD status: has to be declared as integer stat(MPI_STATUS_SIZE) in Fortran MPI_Status Status;in C/C++ Trapezoid rule Trapezoid Method (serial) Trapezoid Method (parallel) Collective communication MPI_BCAST MPI_SCATTER MPI_GATHER MPI_REDUCE https://computing.llnl.gov/tutorials/mpi/ Monte-Carlo Integration multidimensional ‚Äúvolume‚Äù Average value of over integration region Randomly select point in integration volume to determine the average value of , ùíä Error (average determined by N random points) Easy to parallelize","libVersion":"0.5.0","langs":""}